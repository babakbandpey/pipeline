[
    {
        "file": "/app/pipeline/chatbot.py",
        "description": "The Python file `/app/pipeline/chatbot.py` is designed to implement a chatbot pipeline. The primary class in this file is `Chatbot`, which inherits from a base class `Pipeline`. Here's a detailed description of the code and its flow:\n\n1. **Logging Configuration**:\n   - The file starts by configuring the logging settings. The logging level is set to `WARNING` for production use, and a logger instance is created.\n\n2. **Chatbot Class**:\n   - The `Chatbot` class is defined as a subclass of `Pipeline`. This class encapsulates the functionality required for a chatbot pipeline.\n\n3. **invoke Method**:\n   - The `invoke` method is the main entry point for interacting with the chatbot. It takes a `prompt` as input and returns a response.\n   - **Validation and Sanitization**:\n     - The method first validates the `prompt` to ensure it is a non-empty string. If the validation fails, an error message is logged, and an error response is returned.\n     - The `sanitize_input` method is called to sanitize the input prompt. This method removes or escapes potentially harmful characters.\n   - **Invocation and Error Handling**:\n     - The sanitized prompt is then passed to the `invoke` method of the superclass (`Pipeline`).\n     - The method checks if the response object has the expected `content` attribute. If not, an error is logged, and an `AttributeError` is raised.\n     - The method includes exception handling to log and raise any unexpected errors that occur during the invocation process.\n\n4. **sanitize_input Method**:\n   - The `sanitize_input` method is responsible for sanitizing the input string to remove or escape potentially harmful characters. This is a crucial step to ensure the security and integrity of the chatbot pipeline.\n   - The example sanitization logic provided replaces `<` with `&lt;` and `>` with `&gt;`. This logic can be customized based on actual requirements.\n\nIn summary, the code in `/app/pipeline/chatbot.py` is structured to provide a robust and secure chatbot pipeline. The `Chatbot` class handles the main functionality, including input validation, sanitization, and error handling. The `invoke` method is the primary interface for interacting with the chatbot, while the `sanitize_input` method ensures the input is safe for processing."
    },
    {
        "file": "/app/pipeline/py_rag.py",
        "description": "The Python file `/app/pipeline/py_rag.py` contains the implementation of the `PythonRAG` class, which is designed to set up a Retrieval-Augmented Generation (RAG) pipeline for Python code. Below is a detailed description of the flow and functionality of the code:\n\n1. **Class Definition:**\n   - The `PythonRAG` class extends the `Retrieval` class, inheriting its methods and properties. This class is responsible for managing the entire RAG pipeline for Python code.\n\n2. **Initialization (`__init__` method):**\n   - The constructor (`__init__`) initializes the `PythonRAG` object with various configuration parameters passed through `kwargs`.\n   - It sets up instance variables such as `path`, `git_url`, and `exclude`.\n   - If a `git_url` is provided, it calls the `clone_repository` method to clone the repository to the specified path.\n   - It then calls `load_documents` to load Python documents from the filesystem.\n   - Finally, it calls `split_and_store_documents` to split the documents into chunks and set up the vector store.\n\n3. **Cloning Repository (`clone_repository` method):**\n   - This method clones a git repository to the specified path if both `git_url` and `path` are provided.\n   - It validates the `git_url` using the `is_valid_git_url` static method.\n   - If the URL is valid, it attempts to clone the repository using the `Repo.clone_from` method from the `git` library.\n   - Logs are generated to indicate the success or failure of the cloning process.\n\n4. **Validating Git URL (`is_valid_git_url` static method):**\n   - This static method checks if the provided `git_url` starts with \"https://\" or \"git@\" to ensure it is a valid git URL.\n\n5. **Loading Documents (`_load_documents` method):**\n   - This method loads Python documents from the specified filesystem path.\n   - It checks if the path exists and raises an error if it does not.\n   - It uses the `GenericLoader` class from `langchain_community.document_loaders.generic` to load files with a `.py` suffix, excluding any specified in the `exclude` list.\n   - The `LanguageParser` is used to parse the Python files, and the documents are stored in the `self.documents` list.\n   - Logs are generated to indicate the number of documents loaded.\n\n6. **Splitting and Storing Documents (`split_and_store_documents` method):**\n   - This method splits the loaded documents into smaller chunks and sets up the vector store for retrieval.\n   - It uses the `RecursiveCharacterTextSplitter` class from `langchain_text_splitters` to split the documents based on Python language syntax.\n   - The `split_data` method is called to perform the actual splitting, and the resulting chunks are stored in the vector store using the `setup_vector_store` method.\n\nBy following this flow, the `PythonRAG` class effectively sets up a RAG pipeline for Python code, enabling the retrieval and processing of Python documents from a local directory or a git repository."
    },
    {
        "file": "/app/pipeline/chatbot_utils.py",
        "description": "The Python file `/app/pipeline/chatbot_utils.py` contains utility functions and classes designed to support chatbot functionalities. Below is a detailed description of the code and its flow:\n\n1. **Imports and Dependencies**:\n   - The file begins by importing necessary modules such as `json`, `logging`, `re`, `inspect`, and `os`. These modules provide functionalities for JSON handling, logging, regular expressions, and file inspection.\n\n2. **Function: `get_importing_file_name`**:\n   - This function retrieves the name of the file that imports the current module. It uses the `inspect` module to access the call stack and determine the importing file's name.\n\n3. **Class: `ChatbotUtils`**:\n   - This class encapsulates several static utility methods for handling JSON data and URLs, which are essential for chatbot operations.\n\n   - **Method: `clean_and_parse_json`**:\n     - This method cleans and parses poorly formed JSON text into a dictionary. It replaces single quotes with double quotes and ensures that inner quotes are correctly formatted before attempting to parse the text as JSON.\n\n   - **Method: `extract_commands_json`**:\n     - This method extracts a JSON object containing 'commands' and 'command' keys from a given text. It uses regular expressions to identify JSON-like content and attempts to parse it.\n\n   - **Method: `parse_json`**:\n     - This method parses a JSON response and returns the JSON object. If the response is already a dictionary, it returns it directly. Otherwise, it attempts to clean and parse the response string as JSON.\n\n   - **Method: `process_json_response`**:\n     - This method processes a JSON response and returns a markdown list. It converts the JSON data into a formatted markdown list for easier readability.\n\n   - **Method: `extract_json`**:\n     - This method extracts and returns the JSON part from a given response string. It uses regular expressions to find JSON-like content within the response.\n\n   - **Method: `is_valid_url`**:\n     - This method validates a provided URL using a regular expression. It checks if the URL follows the standard format for HTTP, HTTPS, localhost, IPv4, or IPv6 addresses.\n\n   - **Method: `logger`**:\n     - This method sets up and returns a logger instance. It configures the logger with a custom formatter and associates it with the name of the importing file.\n\n4. **Class: `ColoredFormatter`**:\n   - This class extends the `logging.Formatter` class to add color to log messages based on their log level. It defines color codes for different log levels (DEBUG, INFO, WARNING, ERROR, CRITICAL) and applies these colors to the log messages.\n\n   - **Method: `format`**:\n     - This method formats the log record by adding color to the log message based on the log level. It retrieves the original message, applies the appropriate color, and returns the colored message.\n\n5. **Logger Setup**:\n   - The file sets up a logging handler and associates it with the `ColoredFormatter`. It then configures the root logger with this handler and sets the logging level to INFO.\n\nIn summary, the code in `/app/pipeline/chatbot_utils.py` provides utility functions for handling JSON data, validating URLs, and setting up colored logging. The `ChatbotUtils` class contains methods for cleaning, parsing, and extracting JSON data, while the `ColoredFormatter` class enhances log messages with color based on their severity level. The file also includes a function to retrieve the name of the importing file and sets up a logger with a custom formatter."
    },
    {
        "file": "/app/pipeline/config.py",
        "description": "The Python file `/app/pipeline/config.py` is primarily used for configuring the project, particularly focusing on loading and decrypting environment variables, and setting up logging. Here is a detailed description of the flow and functionality of the code:\n\n1. **Importing Modules**:\n   - The code begins by importing necessary modules such as `os`, `logging`, `base64`, and `getpass`.\n   - It also imports cryptographic components from the `cryptography` library, specifically `PBKDF2HMAC`, `hashes`, `default_backend`, and `Fernet`.\n   - The `load_dotenv` function from the `dotenv` module is imported to load environment variables from a `.env` file.\n\n2. **Loading Environment Variables**:\n   - The `load_dotenv()` function is called to load all environment variables from a `.env` file into the environment.\n\n3. **Configuring Logging**:\n   - Logging is configured with a specific logging level (`LOGGING_LEVEL`) set to `logging.INFO`.\n   - The `logging.basicConfig` function is used to set up the logging format, which includes the timestamp, logger name, log level, and message.\n   - A logger instance is created using `logging.getLogger(__name__)`.\n\n4. **Key Derivation Function**:\n   - The `derive_key` function is defined to derive a cryptographic key from a passphrase and a salt using the PBKDF2HMAC algorithm with SHA256 hashing.\n   - This function returns a base64-encoded key.\n\n5. **Decryption Function**:\n   - The `decrypt` function is defined to decrypt an encrypted text using a passphrase.\n   - It first decodes the encrypted text from base64, extracts the salt, and then derives the key using the `derive_key` function.\n   - The `Fernet` class is used to perform the decryption, and the decrypted text is returned. If decryption fails, an error is logged, and `None` is returned.\n\n6. **Passphrase Input**:\n   - The user is prompted to enter a passphrase using `getpass`. If the user enters '0', it indicates that the `.env` file is not encrypted.\n\n7. **Loading API Keys**:\n   - If the passphrase is '0', the code directly loads the `OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, and `AZURE_OPENAI_API_KEY_1` from the environment variables.\n   - If the passphrase is not '0', the code attempts to decrypt these environment variables using the provided passphrase.\n   - The `decrypt` function is called for each environment variable to obtain the decrypted values.\n\n8. **Error Handling and Logging**:\n   - If the passphrase is not provided, an error is logged, and the program exits.\n   - If the `OPENAI_API_KEY` is not set or decryption fails, an error is logged.\n   - If the `OPENAI_API_KEY` is successfully loaded, an informational message is logged.\n\n9. **Version Control Note**:\n   - A comment at the end of the file advises ensuring that the `.env` file is not included in version control by adding it to `.gitignore` or encrypting it using a script.\n\nThis detailed flow outlines the primary functions and logic used in the `config.py` file to manage environment variables, handle encryption and decryption, and set up logging for the project."
    },
    {
        "file": "/app/pipeline/txt_rag.py",
        "description": "The Python file `/app/pipeline/txt_rag.py` contains the implementation of the `TxtRAG` class, which is part of a Retrieval-Augmented Generation (RAG) pipeline. This class extends the `Retrieval` class and is designed to handle text documents, loading them from the filesystem, extracting metadata, splitting them into chunks, and storing them in a vector database for efficient retrieval. Below is a detailed description of the flow of the code:\n\n1. **Class Definition: `TxtRAG`**\n   - The `TxtRAG` class is defined to represent a Python RAG pipeline. It extends the `Retrieval` class, inheriting its methods and properties.\n\n2. **Initialization: `__init__` Method**\n   - The `__init__` method initializes the `TxtRAG` object with configuration parameters passed as keyword arguments (`kwargs`).\n   - It sets the `path` attribute to the directory or file path containing the text documents.\n   - It sets the `auto_clean` attribute to determine whether automatic cleaning of documents is enabled.\n   - It raises a `ValueError` if the `path` parameter is not provided.\n   - It initializes an empty list `documents` to store the loaded documents.\n   - It calls the `check_for_non_ascii_bytes` method to ensure the documents do not contain non-ASCII bytes.\n   - It calls the `load_documents` method to load the documents from the specified path.\n   - It calls the `extract_and_add_metadata` method to extract and add metadata from the documents.\n   - It calls the `split_and_store_documents` method to split the documents into chunks and store them in a vector database.\n\n3. **Loading Documents: `_load_documents` Method**\n   - The `_load_documents` method loads text documents from the filesystem.\n   - If the `path` is a directory, it uses the `DirectoryLoader` class to load all `.txt` files from the directory and its subdirectories.\n   - If the `path` is a single `.txt` file, it uses the `TextLoader` class to load the file.\n   - The loaded documents are stored in the `documents` attribute.\n\n4. **Extracting Metadata: `extract_and_add_metadata` Method**\n   - The `extract_and_add_metadata` method extracts metadata from the documents.\n   - It iterates through each document in the `documents` list.\n   - It checks if the first line of the document's content is a JSON object containing metadata.\n   - If metadata is found, it parses the JSON object and updates the document's metadata.\n   - It removes the metadata line from the document's content.\n\n5. **Splitting and Storing Documents: `split_and_store_documents` Method**\n   - The `split_and_store_documents` method splits the documents into smaller chunks and stores them in a local vector database.\n   - It uses the `RecursiveCharacterTextSplitter` class to split the documents into chunks of a specified size (e.g., 2000 characters) with no overlap.\n   - It calls the `split_data` method to perform the actual splitting.\n   - It calls the `setup_vector_store` method to set up the vector store with the split chunks.\n\nOverall, the `TxtRAG` class provides a comprehensive pipeline for loading, processing, and storing text documents for retrieval-augmented generation tasks. The methods within the class ensure that the documents are properly loaded, metadata is extracted, and the documents are efficiently split and stored for retrieval."
    },
    {
        "file": "/app/pipeline/pipeline.py",
        "description": "The Python file `/app/pipeline/pipeline.py` defines a pipeline for integrating the Ollama model with the Langchain library. The code is structured into several classes and functions, each responsible for different aspects of the pipeline's functionality. Here's a detailed description of the code flow:\n\n### Classes and Their Responsibilities\n\n1. **PipelineConfig**:\n   - **Initialization (`__init__`)**: Sets up initial configurations, including logging and generating a unique session ID.\n   - **Attribute Access (`__getattr__`)**: Provides a way to access attributes dynamically, with a warning if the attribute does not exist.\n   - **Logging Setup (`setup_logging`)**: Configures the logging level for the pipeline.\n   - **Session ID Generation (`generate_session_id`)**: Generates a unique session ID for each pipeline instance.\n\n2. **PipelineSetup (inherits from PipelineConfig)**:\n   - **Initialization (`__init__`)**: Extends the base configuration by setting up chat-related components like chat history, chat prompt, and vector store.\n   - **Chat Prompt Setup (`setup_chat_prompt`)**: Configures the chat prompt template, which includes system messages and user input placeholders.\n   - **Chain Setup (`setup_chain`)**: Creates a chat chain that links the chat prompt with the chat model.\n   - **Chain with Message History Setup (`setup_chain_with_message_history`)**: Configures a chain that includes message history, allowing for context-aware responses.\n\n3. **Pipeline (inherits from PipelineSetup)**:\n   - **Delete Collection (`delete_collection`)**: Deletes the vector store if it exists.\n   - **Add Texts to Vector Store (`add_texts_to_vector_store`)**: Adds text chunks to the vector store, setting it up if it doesn't already exist.\n   - **Invoke (`invoke`)**: Invokes the chatbot with a given prompt, handling any exceptions that occur.\n   - **Clear Chat History (`clear_chat_history`)**: Clears the chat history.\n   - **Sanitize Input (`sanitize_input`)**: Sanitizes user input to prevent injection attacks.\n   - **Modify Chat History (`modify_chat_history`)**: Modifies the chat history by keeping a specified number of messages.\n   - **Summarize Messages (`summarize_messages`)**: Summarizes the chat history and clears the messages.\n   - **Setup Chat (`setup_chat`)**: Initializes the chat model, either using OpenAI, LM Studio, or Ollama, based on the provided configuration.\n   - **Setup Vector Store (`setup_vector_store`)**: Configures the vector store with specified text chunks.\n\n### Static Methods\n\n- **Recursive Character Text Splitter (`recursive_character_text_splitter`)**: Creates a text splitter that divides data into chunks based on specified size and overlap.\n- **Split Data (`split_data`)**: Splits data into chunks using a provided text splitter.\n\n### Key Functionalities\n\n- **Logging and Session Management**: The `PipelineConfig` class handles logging setup and session ID generation, ensuring that each pipeline instance is uniquely identifiable and properly logged.\n- **Chat Configuration**: The `PipelineSetup` class extends the base configuration to include chat-specific setups like chat history, chat prompts, and vector stores.\n- **Chat Operations**: The `Pipeline` class provides methods for interacting with the chat model, including invoking the chatbot, managing chat history, and summarizing messages.\n- **Text Splitting and Vector Store Management**: The static methods and specific functions within the `Pipeline` class handle text splitting and vector store management, ensuring that data is properly chunked and stored for efficient retrieval.\n\n### Error Handling\n\n- **API Connection Errors**: The `setup_chat` method includes error handling for API connection issues, logging errors, and raising exceptions as needed.\n- **General Exceptions**: The `invoke` method and other critical functions include try-except blocks to catch and log any unexpected errors, ensuring that the pipeline can handle failures gracefully.\n\n### Summary\n\nThe code in `/app/pipeline/pipeline.py` is designed to create a robust and flexible pipeline for integrating the Ollama model with the Langchain library. It includes comprehensive configurations, chat operations, text management, and error handling, making it suitable for building advanced chatbot applications."
    },
    {
        "file": "/app/pipeline/nmap_scanner.py",
        "description": "The Python file `/app/pipeline/nmap_scanner.py` is designed to facilitate the execution and parsing of Nmap scans. Below is a detailed description of the code's functionality, which can be used to create a detailed flow of the code:\n\n### NmapScanner Class\n\n#### Attributes:\n- **target**: The target IP address or hostname to scan.\n- **nmap_output**: The raw output of the Nmap scan.\n- **parsed_data**: A dictionary to store parsed data from the Nmap scan.\n\n#### Methods:\n1. **`__init__(self, **kwargs)`**:\n   - Initializes the NmapScanner object.\n   - Requires a target IP address or hostname.\n   - Initializes `nmap_output` and `parsed_data` as empty.\n\n2. **`__getattr__(self, name)`**:\n   - Dynamically retrieves attribute values based on the provided name.\n   - Attributes include `target`, `flags`, `ports`, `firewall`, and `script`.\n   - Raises an `AttributeError` if the attribute does not exist.\n\n3. **`run_nmap(self)`**:\n   - Constructs and runs an Nmap command based on the provided attributes.\n   - Uses the `run_command` method to execute the command.\n   - Returns the output of the Nmap scan.\n\n4. **`run_command(self, command)`**:\n   - Executes a shell command using `subprocess.Popen`.\n   - Logs the command being run.\n   - Captures the output and error streams.\n   - Sets `nmap_output` to the command's output.\n   - Returns the instance itself for method chaining.\n\n5. **`parse_output(self)`**:\n   - Parses the Nmap scan output to extract relevant information.\n   - Uses regular expressions to find and extract:\n     - Host IP address.\n     - Port information (port number, state, service, version).\n     - MAC address and vendor.\n     - OS and CPE information.\n   - Stores the extracted information in `parsed_data`.\n\n6. **`get_parsed_data(self)`**:\n   - Returns the parsed data from the Nmap scan.\n\n7. **`get_output(self)`**:\n   - Returns the raw output of the Nmap scan.\n\n### Example Usage\nThe module provides an example usage of the `NmapScanner` class:\n- Create an instance of `NmapScanner` with a target IP address.\n- Run the Nmap scan using `run_nmap` or `run_command`.\n- Parse the output using `parse_output`.\n- Retrieve the parsed data using `get_parsed_data`.\n\n### Utility Functions\n- **`logger`**: Used for logging information, errors, and debugging messages.\n\n### Regular Expressions\n- **`host_regex`**: Matches the host IP address in the Nmap output.\n- **`port_regex`**: Matches port information (port number, state, service, version).\n- **`mac_regex`**: Matches MAC address and vendor information.\n- **`service_info_regex`**: Matches OS and CPE information.\n\n### Summary\nThe code in `/app/pipeline/nmap_scanner.py` is structured to provide a comprehensive interface for running and parsing Nmap scans. The `NmapScanner` class encapsulates the functionality needed to execute an Nmap scan, parse its output, and retrieve both raw and structured data. The use of regular expressions ensures that relevant information is accurately extracted from the scan results."
    },
    {
        "file": "/app/pipeline/pdf_rag.py",
        "description": "The Python file `/app/pipeline/pdf_rag.py` is designed to handle the loading, processing, and retrieval of PDF documents. It extends the functionality of a base class called `Retrieval` and introduces a specialized class named `PdfRAG`. Below is a detailed description of the flow and functionality of the code:\n\n### Class: PdfRAG\nThe `PdfRAG` class is the main class in this file, and it extends the `Retrieval` class. It is responsible for managing PDF documents, including loading, splitting, and storing them for retrieval purposes.\n\n#### Initialization (`__init__` method)\n- **Parameters**: The constructor accepts various keyword arguments (`kwargs`) that configure the behavior of the class.\n- **Attributes**:\n  - `path`: The file path or directory where the PDF documents are located.\n  - `extract_images`: A boolean flag indicating whether to extract images from the PDFs.\n  - `headers`: Optional headers for loading the PDFs.\n  - `password`: Optional password for encrypted PDFs.\n- **Methods Called**:\n  - `load_documents()`: Loads the PDF documents from the specified path.\n  - `split_and_store_documents()`: Splits the loaded documents into chunks and sets up a vector store for retrieval.\n\n#### Method: `_load_documents`\n- **Functionality**: This method is responsible for loading PDF documents from the filesystem.\n- **Behavior**:\n  - Checks if the provided path is valid.\n  - If the path is a directory, it iterates through all PDF files in the directory and its subdirectories.\n  - If the path is a file, it loads the single PDF file.\n- **Dependencies**:\n  - `PyPDFLoader`: A utility class used to load and split PDF documents.\n- **Logging**: Logs the number of documents loaded and provides a debug log of the documents.\n\n#### Method: `split_and_store_documents`\n- **Functionality**: This method splits the loaded PDF documents into smaller chunks and sets up a vector store for efficient retrieval.\n- **Dependencies**:\n  - `RecursiveCharacterTextSplitter`: A utility class used to split the text into chunks based on character count.\n- **Process**:\n  - Uses `RecursiveCharacterTextSplitter` to split the documents into chunks of a specified size with some overlap.\n  - Calls `setup_vector_store` to store these chunks in a vector store for retrieval.\n\n### Utility Classes and Functions\n- **PyPDFLoader**: This class is used to load and split PDF documents. It handles the extraction of text and images from the PDFs.\n- **RecursiveCharacterTextSplitter**: This class is used to split the text into manageable chunks, which is essential for efficient retrieval and processing.\n\n### Logging\nThe class uses logging to provide information about the number of documents loaded and detailed debug information about the documents themselves.\n\n### Summary\nIn summary, the `PdfRAG` class in the `/app/pipeline/pdf_rag.py` file is designed to manage PDF documents by loading them from a specified path, splitting them into smaller chunks, and storing these chunks in a vector store for efficient retrieval. It leverages utility classes like `PyPDFLoader` and `RecursiveCharacterTextSplitter` to perform these tasks and extends the base `Retrieval` class to integrate with a larger retrieval system."
    },
    {
        "file": "/app/pipeline/rag_factory.py",
        "description": "The Python file `/app/pipeline/rag_factory.py` is designed to facilitate the creation of various Retrieval-Augmented Generation (RAG) objects based on a specified type. This is achieved through a factory design pattern implemented in the `RAGFactory` class. Below is a detailed description of the code and its flow:\n\n1. **Imports and Dependencies**:\n   - The file imports the `importlib` module, which is used for importing other modules programmatically.\n   - It also imports the `Retrieval` class from the `retrieval` module, which serves as a base class or interface for the RAG objects.\n\n2. **RAG Type Mapping**:\n   - A dictionary named `rag_mapping` is defined, which maps string keys representing different RAG types (e.g., 'chat', 'txt', 'py', 'web', 'pdf', 'json') to tuples containing the module name and class name of the corresponding RAG implementation.\n\n3. **RAGFactory Class**:\n   - The `RAGFactory` class is defined as a factory class responsible for creating instances of RAG objects based on the specified type.\n   - The class contains a static method `get_rag_class`.\n\n4. **get_rag_class Method**:\n   - This static method is the core of the factory pattern. It takes a string parameter `_type` which specifies the type of RAG object to create. It also accepts additional keyword arguments (`**kwargs`) that are passed to the constructor of the RAG class.\n   - The method retrieves the module name and class name from the `rag_mapping` dictionary based on the provided `_type`.\n   - If the `_type` is not found in the `rag_mapping`, a `ValueError` is raised indicating that no RAG class is found for the specified type.\n   - The method then uses `importlib.import_module` to import the module dynamically.\n   - It retrieves the class from the imported module using `getattr`.\n   - Finally, it creates and returns an instance of the retrieved class, passing any additional keyword arguments to the class constructor.\n\nIn summary, the file `/app/pipeline/rag_factory.py` provides a flexible and dynamic way to create different types of RAG objects based on a specified type string. The `RAGFactory` class and its `get_rag_class` method handle the logic of mapping type strings to specific RAG implementations, importing the necessary modules, and instantiating the appropriate classes. This design allows for easy extension and maintenance of different RAG types without modifying the core factory logic."
    },
    {
        "file": "/app/pipeline/pipeline_utils.py",
        "description": "The Python file `/app/pipeline/pipeline_utils.py` contains utility functions and classes that facilitate the operation of a chatbot pipeline. Below is a detailed description of the code and its flow:\n\n### Class: `PipelineUtils`\nThis class serves as a utility container for various static methods that assist in setting up and managing the chatbot pipeline.\n\n#### Method: `get_args()`\n- **Purpose**: This method is responsible for parsing command-line arguments.\n- **Functionality**: \n  - It uses `argparse.ArgumentParser` to define various arguments such as `--model`, `--type`, `--path`, `--url`, `--git_url`, `--openai_api_key`, `--example`, `--prompt`, `--system_prompt_template`, `--output_type`, and `--collection_name`.\n  - It returns the parsed arguments.\n\n#### Method: `print_commands_help()`\n- **Purpose**: This method prints a help message that lists available commands for interacting with the chatbot.\n- **Functionality**: \n  - It provides information on commands like `/exit`, `/reset`, `/history`, `/delete`, `/summarize`, `/save`, and `/help`.\n\n#### Method: `print_examples()`\n- **Purpose**: This method prints example commands to demonstrate how to run the script.\n- **Functionality**: \n  - It shows various example usages of the script with different types and parameters.\n\n#### Method: `create_chatbot(args)`\n- **Purpose**: This method creates and returns a chatbot instance based on the provided arguments.\n- **Functionality**: \n  - It first checks if the `--example` flag is set and prints examples if true.\n  - It then calls `get_kwargs(args)` to get keyword arguments.\n  - Depending on the `--type` argument, it returns an appropriate chatbot instance using `RAGFactory.get_rag_class`.\n\n#### Method: `get_base_url_and_api_key(args)`\n- **Purpose**: This method retrieves the base URL and API key based on the model specified in the arguments.\n- **Functionality**: \n  - It sets the URL endpoint and API key for different models like `llama3`, `phi3`, `gpt`, `azure`, and `lmstudio`.\n  - It returns the URL endpoint and API key.\n\n#### Method: `get_kwargs(args)`\n- **Purpose**: This method constructs and returns a dictionary of keyword arguments based on the provided arguments.\n- **Functionality**: \n  - It calls `get_base_url_and_api_key(args)` to get the base URL and API key.\n  - It includes additional parameters like `collection_name`, `git_url`, `path`, and `url`.\n  - It returns the constructed dictionary.\n\n#### Method: `handle_command(prompt, chatbot)`\n- **Purpose**: This method handles various commands issued to the chatbot.\n- **Functionality**: \n  - It defines several helper functions like `save_chat_history()`, `default_action()`, `exit_chat()`, `show_history()`, and `delete_message()`.\n  - It uses a dictionary to map commands to their corresponding functions.\n  - It retrieves and executes the appropriate function based on the command.\n\n### Helper Functions\n- **`save_chat_history()`**: Saves the chat history to a file.\n- **`default_action()`**: Invokes the chatbot with the given prompt.\n- **`exit_chat()`**: Exits the chatbot.\n- **`show_history()`**: Displays the chat history.\n- **`delete_message()`**: Deletes a specified message from the chat history.\n\n### Imports\n- The file imports various modules and functions such as `datetime`, `os`, `secrets`, `sys`, `argparse`, and custom modules like `OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY_1`, `RAGFactory`, `Retrieval`, and `logger`.\n\n### Summary\nThe `PipelineUtils` class in `/app/pipeline/pipeline_utils.py` is designed to streamline the setup and management of a chatbot pipeline. It provides methods for parsing command-line arguments, creating chatbot instances, handling commands, and managing chat history. The class leverages various helper functions and external modules to achieve its functionality."
    },
    {
        "file": "/app/pipeline/retrieval.py",
        "description": "The Python file `/app/pipeline/retrieval.py` contains the implementation of a class named `Retrieval`, which is part of a chatbot pipeline designed to retrieve documents and answer questions based on those documents. Below is a detailed description of the code and its flow:\n\n### Class: `Retrieval`\nThe `Retrieval` class inherits from a base class named `Pipeline`. It is designed to set up and manage a chatbot pipeline that retrieves documents and provides answers based on the retrieved information.\n\n#### Methods:\n\n1. **`setup_chat_prompt(self, system_template=None, output_type=None)`**:\n   - This method sets up the prompt for the chatbot. It takes an optional `system_template` parameter, which is a string template for the system's response format. If no template is provided, a default template is used.\n   - The method ensures that the `system_template` contains a placeholder for the context and raises a `ValueError` if the template is not a string.\n   - It calls the `setup_chat_prompt` method of the parent class (`Pipeline`) to complete the setup.\n\n2. **`setup_chain(self, search_type=None, search_kwargs=None)`**:\n   - This method sets up the chatbot pipeline chain, which includes a series of processing steps.\n   - It validates the `search_type` parameter, which determines the type of search to use (e.g., \"mmr\", \"similarity\", \"similarity_score_threshold\"). If no `search_type` is provided, it defaults to \"mmr\".\n   - It validates the `search_kwargs` parameter, which is a dictionary of keyword arguments for the search. If no `search_kwargs` are provided, default values are used.\n   - The method creates a prompt template using `ChatPromptTemplate.from_messages` and sets up a retriever using `self.vector_store.as_retriever`.\n   - It creates a retrieval chain by combining a history-aware retriever and a document combination chain using `create_history_aware_retriever` and `create_stuff_documents_chain`.\n\n3. **`_load_documents(self)`**:\n   - This is an abstract method that must be implemented by subclasses. It is responsible for loading documents from the filesystem and initializing the `documents` attribute.\n\n4. **`invoke(self, prompt) -> str`**:\n   - This method invokes the chatbot with a specified query (`prompt`).\n   - It sanitizes the input prompt and adds it to the chat history.\n   - It calls the `invoke` method of the parent class (`Pipeline`) to get a response and extracts the answer from the response.\n   - The answer is added to the chat history, and the method returns the answer.\n\n5. **`check_for_non_ascii_bytes(self)`**:\n   - This method checks for non-ASCII bytes in a text file or directory.\n   - It defines a nested function `detect_and_clean` that logs the file being checked, finds non-ASCII bytes using `FileUtils.find_non_ascii_bytes`, and either cleans the file automatically or raises a `ValueError` if non-ASCII bytes are found.\n   - The method iterates through files in the specified path and calls `detect_and_clean` for each file.\n\n6. **`load_documents(self)`**:\n   - This method loads Python documents from the filesystem by calling the `_load_documents` method.\n   - It handles various exceptions such as `UnicodeDecodeError`, `ValueError`, `FileNotFoundError`, and `PermissionError`, logging appropriate error messages.\n\n### Error Handling:\n- The code includes extensive error handling to manage various exceptions that may occur during the loading of documents and the setup of the chatbot pipeline. It logs error messages and raises appropriate exceptions when necessary.\n\n### Logging:\n- The class uses a logger (`self.logger`) to log information, warnings, and errors throughout the methods.\n\n### Utilities:\n- The class relies on utility functions from `FileUtils` for tasks such as finding and cleaning non-ASCII bytes in files.\n\n### Summary:\nThe `Retrieval` class in `/app/pipeline/retrieval.py` is designed to set up and manage a chatbot pipeline that retrieves documents and answers questions based on those documents. It includes methods for setting up chat prompts, creating processing chains, loading documents, invoking the chatbot, and checking for non-ASCII bytes in files. The class also includes robust error handling and logging to ensure smooth operation."
    },
    {
        "file": "/app/pipeline/file_utils.py",
        "description": "The Python file `/app/pipeline/file_utils.py` contains a utility class named `FileUtils` that provides various static methods for performing file operations. Below is a detailed description of the functionalities provided by this class:\n\n1. **Class Definition:**\n   - `FileUtils`: This is a utility class designed to encapsulate various file operations. All methods within this class are static, meaning they can be called on the class itself without needing an instance.\n\n2. **Methods:**\n\n   - **`get_files(root_path=\".\", extension=\".py\", exclude_dirs=[\"env\", \".git\"]) -> list`**:\n     - This method is used to retrieve all files with a specified extension (default is `.py`) from a given root directory. It excludes certain directories (like `env` and `.git`) from the search. The method returns a list of file paths that match the criteria.\n\n   - **`write_to_file(file_path, content, mode='w', encoding='utf-8')`**:\n     - This method writes or appends content to a file. It supports three modes: 'w' for writing (overwriting) the file, 'a' for appending to the file, and 'x' for exclusive creation (which raises an error if the file already exists). The method also logs the success or failure of the operation.\n\n   - **`read_file(file_path, encoding='utf-8') -> str`**:\n     - This method reads the content of a file and returns it as a string. It uses the specified encoding (default is `utf-8`). If the file is not found or another error occurs, it logs the error and returns an empty string.\n\n   - **`prepend_to_file(file_path, content, encoding='utf-8')`**:\n     - This method prepends content to a file. If the file does not exist, it creates it and writes the content. If the file exists, it reads the current content, prepends the new content, and writes it back to the file. The method logs the success or failure of the operation.\n\n   - **`clean_non_ascii_bytes(file_path, replacement_byte=b' ')`**:\n     - This method cleans non-ASCII bytes from a text file by replacing them with a specified byte (default is a space). It reads the file in binary mode, replaces non-ASCII bytes, and writes the cleaned data back to the file. The method logs the success or failure of the operation.\n\n   - **`find_non_ascii_bytes(file_path)`**:\n     - This method finds and returns the positions and values of non-ASCII bytes in a text file. It reads the file in binary mode and identifies bytes with values greater than 0x7F. The method returns a list of tuples containing the position and byte value of each non-ASCII byte.\n\n3. **Logging:**\n   - The methods in the `FileUtils` class use a logger (imported from `chatbot_utils`) to log information about the success or failure of file operations. This helps in tracking the flow of operations and debugging issues.\n\n4. **Error Handling:**\n   - The methods include error handling mechanisms to catch and log exceptions such as `FileNotFoundError` and other general exceptions. This ensures that the program can handle errors gracefully and provide useful information for troubleshooting.\n\nIn summary, the `FileUtils` class in `/app/pipeline/file_utils.py` provides a comprehensive set of static methods for file operations, including reading, writing, appending, and cleaning files. It also includes functionality for finding specific byte patterns and handles errors and logging to facilitate smooth operation and debugging."
    },
    {
        "file": "/app/pipeline/web_rag.py",
        "description": "The Python file `/app/pipeline/web_rag.py` contains the implementation of the `WebRAG` class, which is designed to create a pipeline for a chatbot that retrieves documents from a website and answers questions based on those documents. Below is a detailed description of the code and its flow:\n\n### Class: WebRAG\nThe `WebRAG` class inherits from the `Retrieval` class and serves as the main component of the pipeline. It is responsible for initializing the pipeline, loading documents from a specified URL, splitting the text data, and setting up a vector store for efficient retrieval.\n\n#### Initialization (`__init__` method)\n- **Parameters**: The constructor accepts various keyword arguments, including `base_url` and `model`.\n- **Validation**: It validates the provided URL using the `ChatbotUtils.is_valid_url` method. If the URL is invalid, it raises a `ValueError`.\n- **Document Loading**: It calls the `load_documents` method to load documents from the specified URL.\n- **Text Splitting**: It uses a text splitter, obtained from the `recursive_character_text_splitter` method, to split the loaded documents into manageable chunks.\n- **Vector Store Setup**: It sets up a vector store using the `setup_vector_store` method, which facilitates efficient document retrieval.\n\n#### Document Loading (`_load_documents` method)\n- **Loader Initialization**: It initializes a `WebBaseLoader` with the specified URL.\n- **Data Loading**: It loads the documents from the URL using the `load` method of `WebBaseLoader`.\n- **Error Handling**: If an error occurs during the loading process, it logs the exception and raises it.\n\n### Supporting Classes and Methods\n- **WebBaseLoader**: This class is part of the `langchain_community.document_loaders` module and is used to load documents from a web URL.\n- **Retrieval**: The parent class of `WebRAG`, which likely contains methods and attributes for document retrieval and processing.\n- **ChatbotUtils**: A utility class that provides helper methods, such as `is_valid_url`, to validate URLs and possibly other utility functions.\n- **recursive_character_text_splitter**: A method that returns a text splitter, which is used to divide the loaded documents into smaller chunks.\n- **split_data**: A method that takes the text splitter and the loaded documents to create chunks of text.\n- **setup_vector_store**: A method that sets up a vector store using the text chunks, enabling efficient retrieval of relevant documents.\n\n### Error Handling\nThroughout the code, there are try-except blocks to catch and log exceptions. This ensures that any issues during the initialization, document loading, or other processes are properly logged and raised, making debugging easier.\n\n### Logging\nThe class uses a logger to record significant events and errors, which is crucial for monitoring the pipeline's performance and troubleshooting issues.\n\n### Summary\nThe `WebRAG` class in the `/app/pipeline/web_rag.py` file is designed to create a robust pipeline for a chatbot that retrieves and processes documents from a web URL. It involves URL validation, document loading, text splitting, and setting up a vector store for efficient retrieval. The class is built with error handling and logging mechanisms to ensure reliability and ease of debugging."
    },
    {
        "file": "/app/pipeline/json_rag.py",
        "description": "The Python file `/app/pipeline/json_rag.py` contains the implementation of the `JsonRAG` class, which is designed to handle Retrieval-Augmented Generation (RAG) for JSON documents. This class extends the `Retrieval` class and provides functionalities to load, process, and store JSON documents for efficient retrieval. Below is a detailed description of the flow and functionalities provided by the code:\n\n1. **Class Definition:**\n   - The `JsonRAG` class is defined to represent a Python RAG pipeline specifically for JSON documents. It inherits from the `Retrieval` class, which likely provides core retrieval functionalities.\n\n2. **Initialization (`__init__` method):**\n   - The `__init__` method initializes the `JsonRAG` object with configuration parameters passed via `kwargs`.\n   - It sets the `path` attribute to the location of the JSON files and an `auto_clean` attribute to determine if automatic cleaning is needed.\n   - It raises a `ValueError` if the `path` parameter is not provided.\n   - The method initializes an empty list `documents` to store the loaded JSON documents.\n   - It calls `check_for_non_ascii_bytes` to ensure the documents do not contain non-ASCII bytes.\n   - It calls `load_documents` to load the JSON documents from the specified path.\n   - It calls `split_and_store_documents` to split the documents into chunks and store them in a vector store for efficient retrieval.\n\n3. **Loading Documents (`_load_documents` method):**\n   - The `_load_documents` method is responsible for loading JSON documents from the filesystem.\n   - If the specified path is a directory, it uses `DirectoryLoader` with a glob pattern to load all JSON files in the directory and its subdirectories.\n   - If the specified path is a single JSON file, it uses `JSONLoader` to load the content of the file.\n   - The loaded documents are stored in the `documents` attribute.\n\n4. **Splitting and Storing Documents (`split_and_store_documents` method):**\n   - The `split_and_store_documents` method handles the splitting of documents into smaller chunks and sets up a vector store for efficient retrieval.\n   - It logs the start of the splitting and storing process.\n   - It uses `RecursiveCharacterTextSplitter` to split the documents into chunks of a specified size (e.g., 2000 characters) with no overlap.\n   - It calls `split_data` to perform the actual splitting of the documents.\n   - It calls `setup_vector_store` to store the resulting chunks in a local vector database, making them ready for retrieval.\n\n5. **Additional Functionalities:**\n   - The class likely includes other methods inherited from the `Retrieval` class, which are not detailed in the provided context but are essential for the overall RAG pipeline.\n\nIn summary, the `JsonRAG` class in `/app/pipeline/json_rag.py` is designed to load JSON documents from a specified path, split them into manageable chunks, and store them in a vector store for efficient retrieval. The class leverages `DirectoryLoader`, `JSONLoader`, and `RecursiveCharacterTextSplitter` to achieve these tasks, ensuring that the documents are processed and stored in a way that facilitates quick and accurate retrieval."
    },
    {
        "file": "/app/pipeline/__init__.py",
        "description": "The Python file `/app/pipeline/__init__.py` serves as the initialization module for the `pipeline` package. This file is crucial for setting up the package and making its components easily accessible when the package is imported. Below is a detailed description of what the code in this file does:\n\n1. **Import Statements**:\n   - The file begins by importing various classes and functions from other modules within the `pipeline` package. These imports include:\n     - `OPENAI_API_KEY` from `config`\n     - `Chatbot` from `chatbot`\n     - `TxtRAG`, `WebRAG`, `PyRAG`, `PdfRAG`, `JsonRAG` from their respective modules\n     - `PipelineUtils` from `pipeline_utils`\n     - `FileUtils` from `file_utils`\n     - `ChatbotUtils` and `logger` from `chatbot_utils`\n     - `NmapScanner` from `nmap_scanner`\n\n2. **`__all__` List**:\n   - The `__all__` list is defined to specify the public interface of the module. This list includes the names of the classes, functions, and variables that should be accessible when the module is imported. The list contains:\n     - `'OPENAI_API_KEY'`\n     - `'PipelineUtils'`\n     - `'FileUtils'`\n     - `'ChatbotUtils'`\n     - `'Chatbot'`\n     - `'TxtRAG'`\n     - `'WebRAG'`\n     - `'PyRAG'`\n     - `'PdfRAG'`\n     - `'JsonRAG'`\n     - `'logger'`\n     - `'NmapScanner'`\n\n3. **Metadata**:\n   - The file also includes metadata about the package:\n     - `__version__` specifies the current version of the package, which is `'0.5.0'`.\n     - `__author__` provides the author's name and contact information, which is `'Babak Bandpey <bb@cocode.dk>'`.\n\n### Detailed Flow of the Code:\n\n1. **Configuration**:\n   - The `OPENAI_API_KEY` is imported from the `config` module, which likely contains the API key required for interacting with OpenAI's services.\n\n2. **Chatbot Functionality**:\n   - The `Chatbot` class is imported from the `chatbot` module. This class is likely responsible for managing the chatbot's interactions and logic.\n\n3. **Retrieval-Augmented Generation (RAG) Components**:\n   - Several RAG-related classes are imported:\n     - `TxtRAG` for handling text-based retrieval-augmented generation.\n     - `WebRAG` for web-based retrieval-augmented generation.\n     - `PyRAG` for Python code-based retrieval-augmented generation.\n     - `PdfRAG` for PDF-based retrieval-augmented generation.\n     - `JsonRAG` for JSON-based retrieval-augmented generation.\n\n4. **Utility Functions**:\n   - `PipelineUtils` and `FileUtils` are utility classes that provide various helper functions for the pipeline and file operations, respectively.\n\n5. **Chatbot Utilities**:\n   - `ChatbotUtils` likely contains additional helper functions specifically for the chatbot.\n   - `logger` is probably a logging utility to facilitate debugging and monitoring.\n\n6. **Network Scanning**:\n   - The `NmapScanner` class is imported from the `nmap_scanner` module, which suggests functionality for network scanning using Nmap.\n\nBy organizing these components in the `__init__.py` file, the `pipeline` package ensures that all essential classes, functions, and variables are readily available for import, making it easier to build and manage a chatbot application with various retrieval-augmented generation capabilities."
    },
    {
        "file": "/app/scripts/code_guard.py",
        "description": "The Python script located at `/app/scripts/code_guard.py` is designed to scan a codebase for potential issues and report them back to the user. The script specifically targets Python files, excluding the `env` directory. Below is a detailed description of the flow and functionality of the code:\n\n1. **Imports and Dependencies**:\n   - The script imports several modules: `os` for interacting with the operating system, `datetime` for handling date and time, and custom modules `PipelineUtils`, `FileUtils`, and `logger` from a `pipeline` package.\n\n2. **Main Function**:\n   - The `main()` function serves as the entry point of the script. It orchestrates the entire scanning process.\n\n3. **Argument Parsing**:\n   - The script uses `PipelineUtils.get_args()` to parse command-line arguments. It sets the argument type to \"py\" to indicate that it will be scanning Python files.\n\n4. **File Collection**:\n   - If a specific path is provided via command-line arguments, the script uses `FileUtils.get_files_from_path(args.path, \".py\")` to collect all Python files from the specified path.\n   - If no path is provided, it defaults to collecting all Python files in the current directory using `FileUtils.get_files()`.\n\n5. **Output File Creation**:\n   - An output file is created with a timestamp in its name, stored in the `./history/` directory. This file will store the analysis results.\n\n6. **File Analysis Loop**:\n   - The script iterates over each collected Python file.\n   - For each file, it sets the absolute path of the file to `args.path`.\n\n7. **Chatbot Initialization**:\n   - A chatbot instance is created using `PipelineUtils.create_chatbot(args)`. This chatbot is responsible for analyzing the code.\n\n8. **Logging and Analysis**:\n   - The script logs the start of the analysis for each file using `logger.info()`.\n   - It appends a header to the output file indicating the start of analysis for the current file.\n\n9. **Code Analysis**:\n   - The chatbot invokes a series of commands to analyze the code:\n     - It first checks for potential issues and security vulnerabilities, specifying line numbers and issues.\n     - It then suggests improvements for the code.\n     - Finally, it scores the code on a scale of 1 to 10.\n\n10. **Appending Results**:\n    - The responses from the chatbot are appended to the output file after each analysis step.\n\n11. **Cleanup**:\n    - After analyzing each file, the script deletes the chatbot's collection and clears its chat history to prepare for the next file.\n\n12. **Execution**:\n    - The script is executed by calling the `main()` function within the `if __name__ == \"__main__\":` block.\n\nIn summary, the script automates the process of scanning Python files for issues, suggesting improvements, and scoring the code. It leverages a chatbot for the analysis and logs the results in a timestamped markdown file."
    },
    {
        "file": "/app/scripts/code_chart.py",
        "description": "The Python file `/app/scripts/code_chart.py` is designed to scan a codebase for potential issues and report them back to the user. The script specifically targets Python files, excluding the `env` directory. Here is a detailed description of the code's functionality and flow:\n\n1. **Imports and Logging Configuration**:\n   - The script imports necessary modules such as `os`, `logging`, and `datetime`.\n   - It also imports utility classes `PipelineUtils` and `FileUtils` from a module named `pipeline`.\n   - Logging is configured to display information-level messages with timestamps.\n\n2. **Main Function**:\n   - The `main()` function serves as the entry point of the script.\n   - It retrieves command-line arguments using `PipelineUtils.get_args()`.\n   - The script sets the argument type to \"py\" to focus on Python files.\n\n3. **File Retrieval**:\n   - If a specific path is provided in the arguments, the script uses `FileUtils.get_files_from_path(args.path, \".py\")` to get all Python files from that path.\n   - If no path is provided, it defaults to retrieving all Python files using `FileUtils.get_files()`.\n\n4. **Output File Creation**:\n   - An output file is created with a timestamp in its name, stored in the `./history/` directory. This file will store the analysis results.\n\n5. **File Analysis Loop**:\n   - The script iterates over each Python file retrieved.\n   - For each file, it gets the absolute path and updates the `args.path` with this path.\n   - A chatbot instance is created using `PipelineUtils.create_chatbot(args)`.\n\n6. **Code Analysis**:\n   - The script logs the start of analysis for each file and appends this information to the output file.\n   - It invokes the chatbot to analyze the code and write a description of what the code does. The response is appended to the output file.\n   - It then invokes the chatbot again to write a description that could be used for creating a detailed flow chart. This response is also appended to the output file.\n\n7. **Cleanup**:\n   - After analyzing each file, the script deletes the chatbot's collection and clears its chat history to prepare for the next file.\n\n8. **Execution**:\n   - The `main()` function is called if the script is executed as the main module.\n\nThis script automates the process of analyzing Python code files and generating descriptive reports, which can be useful for understanding the codebase and creating detailed flow charts."
    },
    {
        "file": "/app/scripts/create_script.py",
        "description": "The Python file `/app/scripts/create_script.py` is designed to modify the content of an existing script file based on a given prompt. Here's a detailed description of the flow and functionality of the code:\n\n1. **Imports and Dependencies**:\n   - The script imports the `PipelineUtils` module, which is presumably a utility module that provides various helper functions and classes for handling pipelines and chatbot interactions.\n\n2. **Function: `create_script`**:\n   - **Parameters**:\n     - `path` (str): The file path to the script that needs to be modified.\n     - `prompt` (str): The information or instructions that will be used to modify the script.\n   - **Functionality**:\n     - The function starts by reading and printing the content of the script located at the provided `path`.\n     - It then defines a system template that instructs the language model (LLM) to modify the script based on the provided information. The template emphasizes that the returned file should be in Python and well-formatted.\n     - The function retrieves arguments using `PipelineUtils.get_args()` and sets the type to 'py' and the path to the provided script path.\n     - A chatbot instance is created using `PipelineUtils.create_chatbot(args)`.\n     - The chatbot is set up with the defined system template and an output type of 'python'.\n     - The chatbot is then invoked with the provided prompt to generate the modified script.\n     - The modified script (or the response from the chatbot) is logged using the chatbot's logger.\n     - The function returns the modified script.\n\n3. **Function: `main`**:\n   - **Functionality**:\n     - This is the main entry point of the script.\n     - It defines the path to a test script located at `./tests/data/test.py`.\n     - It calls the `create_script` function with the path to the test script and a prompt to write test cases for each method in the test script.\n\n4. **Execution**:\n   - The script includes a standard Python entry point check (`if __name__ == \"__main__\":`), which ensures that the `main` function is executed when the script is run directly.\n\nIn summary, the script is designed to read an existing Python script, use a language model to modify the script based on a given prompt, and return the modified script. The main function demonstrates this process by specifying a test script and a prompt to generate test cases."
    },
    {
        "file": "/app/scripts/run.py",
        "description": "The Python file `./run.py` is designed to serve as an entry point for a chatbot application. Below is a detailed description of the code's functionality and flow:\n\n1. **Imports and Author Information**:\n   - The file begins with a comment indicating the author, Babak Bandpey, and the purpose of the file, which is to try out the classes.\n   - It imports the `datetime` module to handle date and time operations.\n   - It also imports `PipelineUtils` from the `pipeline` module, which contains utility functions and classes necessary for the chatbot's operation.\n\n2. **Main Function**:\n   - The `main()` function is defined as the central function that orchestrates the chatbot's workflow.\n   - It starts by printing a welcome message to the user and displaying the current date and time using `datetime.datetime.now()`.\n\n3. **Argument Parsing and Chatbot Initialization**:\n   - The `main()` function calls `PipelineUtils.get_args()` to parse any command-line arguments that might be needed for the chatbot's configuration.\n   - It then calls `PipelineUtils.create_chatbot(args)` to create an instance of the chatbot using the parsed arguments.\n\n4. **Chat Loop**:\n   - The function enters a `try` block with an infinite `while` loop to continuously interact with the user.\n   - Inside the loop, it waits for user input. If `next_prompt` is `None`, it prompts the user to enter a message. Otherwise, it uses the value of `next_prompt`.\n   - The user input is then passed to `PipelineUtils.handle_command()`, which processes the input and returns the next prompt or response from the chatbot.\n\n5. **Graceful Exit**:\n   - The loop is designed to run indefinitely until interrupted by the user (e.g., by pressing `Ctrl+C`).\n   - When a `KeyboardInterrupt` exception is caught, the program prints a goodbye message and exits gracefully.\n\n6. **Entry Point**:\n   - The script includes a standard Python entry point check (`if __name__ == \"__main__\":`), which ensures that the `main()` function is called only when the script is executed directly, not when it is imported as a module.\n\nIn summary, the `./run.py` file sets up a chatbot application by initializing necessary components, entering a loop to handle user interactions, and providing a mechanism for graceful termination. The key functions and classes used are `PipelineUtils.get_args()`, `PipelineUtils.create_chatbot()`, and `PipelineUtils.handle_command()`."
    },
    {
        "file": "/app/scripts/push.py",
        "description": "The Python script located at `/app/scripts/push.py` is designed to automate the process of committing changes to a Git repository. Below is a detailed description of the flow and functionality of the code, referring to the functions and classes used:\n\n1. **Imports and Dependencies**:\n   - The script imports several modules including `subprocess`, `sys`, `os`, `datetime`, and custom modules from `pipeline` such as `PipelineUtils`, `ChatbotUtils`, and `logger`.\n\n2. **Utility Function: `run_command`**:\n   - This function runs a shell command using `subprocess.run` and returns the output. It handles errors by logging exceptions and exiting the script with the appropriate return code.\n\n3. **Check for Changes: `has_changes_to_commit`**:\n   - This function checks if there are any changes to commit by running `git status --porcelain`. It returns a boolean indicating whether there are changes.\n\n4. **Get Current Branch: `get_current_branch_name`**:\n   - This function retrieves the current Git branch name using `git rev-parse --abbrev-ref HEAD`.\n\n5. **Create Commit Message: `create_commit_message`**:\n   - This function interacts with a chatbot to generate a detailed commit message. It formats the message as JSON and includes a title, description, and type. The message is saved to a file named `commit_message.txt`.\n\n6. **Main Function: `main`**:\n   - The main function orchestrates the entire process:\n     - It first checks if there are any changes to commit.\n     - It retrieves the current branch name and handles the case where the user is on the `main` branch by creating and checking out a new branch.\n     - It stages all changes using `stage_changes`.\n     - It generates a `diff.txt` file containing all changes.\n     - It runs tests using `run_tests` and aborts if they fail.\n     - It creates a chatbot instance and runs various checks (security, vulnerability, code quality, pylint) using `run_checks`.\n     - It generates a commit message and asks for user confirmation before proceeding.\n     - It performs the Git commit and pushes the changes to the upstream branch.\n     - It cleans up temporary files.\n\n7. **Run Tests: `run_tests`**:\n   - This function runs tests using `pytest` and aborts the commit if any tests fail.\n\n8. **Stage Changes: `stage_changes`**:\n   - This function stages all changes, including untracked files, using `git add -A`.\n\n9. **Unstage Changes: `unstage_changes`**:\n   - This function unstages all changes using `git reset`.\n\n10. **Generate Diff: `generate_diff`**:\n    - This function generates a `diff.txt` file containing all staged changes using `git diff --staged`.\n\n11. **Branch Existence Check: `branch_exists`**:\n    - This function checks if a branch already exists by listing all branches and checking for the presence of the specified branch name.\n\n12. **Create and Checkout New Branch: `create_and_checkout_new_branch`**:\n    - This function creates a new branch and switches to it. It handles user input for the branch name and ensures the name is not `main` or already existing.\n\n13. **Cleanup: `cleanup`**:\n    - This function removes temporary files (`diff.txt` and `commit_message.txt`) created during the process.\n\n14. **Run Checks: `run_checks`**:\n    - This function runs various checks (security, vulnerability, code quality, pylint) using the chatbot and asks for user confirmation for each check.\n\n15. **Improvement Check**:\n    - The script also includes a section for suggesting code improvements using the chatbot. It asks the user if they want to implement the suggested improvements.\n\nOverall, the script automates the process of committing changes to a Git repository by performing a series of checks and interactions with a chatbot to ensure code quality and security."
    },
    {
        "file": "/app/scripts/codify.py",
        "description": "The Python file `/app/scripts/codify.py` is designed to function as a simple AI agent that processes text to codify it into policies and standards. Below is a detailed description of the code's flow and the functions and classes it utilizes:\n\n1. **Main Function (`main`)**:\n   - The script begins execution in the `main` function.\n   - It first attempts to retrieve command-line arguments using `PipelineUtils.get_args()`.\n   - A chatbot instance is created using `PipelineUtils.create_chatbot(args)`.\n   - The chatbot is then invoked with a specific prompt to identify the subjects covered in the content. The expected response is formatted as a Python dictionary.\n\n2. **Processing the Chatbot's Answer (`process_answer`)**:\n   - The `process_answer` function is called with the chatbot's answer and the chatbot instance.\n   - This function attempts to clean and parse the chatbot's response into a JSON object.\n   - If the response is not valid JSON, an error message is printed.\n   - The function iterates over the keys in the parsed JSON object. For each key, it further iterates over the values.\n   - For each value, the chatbot is invoked again to find all the requirements related to that value in the content, formatted as an ordered bullet list.\n   - The results are printed, and the user is prompted to decide whether to continue or stop the process.\n\n3. **User Interaction Loop**:\n   - The script enters an infinite loop where it continuously prompts the user for input.\n   - The user's input is handled by `PipelineUtils.handle_command`, which processes the command and interacts with the chatbot accordingly.\n   - The loop can be interrupted by a `KeyboardInterrupt` (e.g., pressing Ctrl+C), which gracefully exits the program and logs a \"Goodbye!\" message using `ChatbotUtils.logger().exception`.\n\n4. **Error Handling**:\n   - The script includes error handling for JSON decoding errors and other potential exceptions during the processing of the chatbot's responses.\n   - Errors are logged using `ChatbotUtils.logger().exception`.\n\n5. **Imports**:\n   - The script imports necessary modules such as `json` for JSON handling and custom utility modules `PipelineUtils` and `ChatbotUtils` for various helper functions and logging.\n\nIn summary, the script is structured to:\n- Initialize and configure a chatbot.\n- Interact with the chatbot to extract and process information from the content.\n- Continuously engage with the user to refine and display the extracted information.\n- Handle errors and interruptions gracefully."
    },
    {
        "file": "/app/scripts/organizer.py",
        "description": "The Python script located at `/app/scripts/organizer.py` is designed to read the content of files, analyze it, and organize it in a structured manner. Below is a detailed description of the flow and functionality of the code:\n\n1. **Imports and Dependencies**:\n   - The script imports necessary modules and functions, including `Union` from `typing`, `datetime`, and utility classes from `pipeline` and `ChatbotUtils`.\n\n2. **Function: `analyzer`**:\n   - This function takes a `chatbot` object and a `prompt` string as input.\n   - It logs the prompt and invokes the chatbot with the given prompt, requesting the output to be formatted as JSON.\n   - The response from the chatbot is logged and then parsed into a JSON format using `ChatbotUtils.parse_json`.\n   - The function returns the parsed JSON response.\n\n3. **Function: `organize_content`**:\n   - This is the main function responsible for organizing the content of files.\n   - It retrieves files from a specified path and of a specified type (either `.pdf` or `.txt`) using `FileUtils.get_files_from_path`.\n   - For each file, it creates an output file with a timestamp in the filename.\n   - It sets up a collection name for the chatbot and creates a chatbot instance using `PipelineUtils.create_chatbot`.\n   - It logs the start of processing and constructs a prompt to analyze the content and list the areas covered.\n   - The `analyzer` function is called with this prompt, and the areas covered are extracted from the response.\n   - For each area covered, it constructs another prompt to list the relevant requirements needed to comply with that area.\n   - The `analyzer` function is called again to get these requirements, which are then written to the output file.\n   - It constructs a prompt to write the purpose of the policies and requirements, calls the `analyzer` function, and prepends this information to the output file.\n   - It constructs another prompt to summarize the content, calls the `analyzer` function, and prepends the summary to the output file.\n   - Finally, it deletes the chatbot collection and clears the chat history.\n\n4. **Function: `main`**:\n   - This is the entry point of the script.\n   - It retrieves arguments using `PipelineUtils.get_args`.\n   - It checks if the file type is either 'pdf' or 'txt' and raises a ValueError if not.\n   - It calls the `organize_content` function within a try-except block to handle keyboard interruptions gracefully.\n\n5. **Script Execution**:\n   - The script checks if it is being run as the main module and calls the `main` function to start the process.\n\nIn summary, the script reads files, analyzes their content using a chatbot, and organizes the information into a structured markdown file. The key functions involved are `analyzer` for analyzing content, `organize_content` for orchestrating the entire process, and `main` for initializing and handling script execution."
    },
    {
        "file": "/app/scripts/__init__.py",
        "description": "I don't know. The provided context does not contain any information about the contents of the `/app/scripts/__init__.py` file. It only mentions the import statement for `pipeline.pipeline_utils` and the `__all__` declaration."
    },
    {
        "file": "/app/scripts/readme_writer.py",
        "description": "I don't know."
    }
]